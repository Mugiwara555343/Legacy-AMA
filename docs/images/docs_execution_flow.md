from pathlib import Path

# Updated markdown content reflecting the use of text-generation-webui instead of LM Studio
updated_md_content = """# 🧠 Memory Execution Flow – Local Model Test Walkthrough

This document illustrates the local execution of a parsed memory entry within the AI architecture.  
It walks through the memory file journey: from input log → parsing → model routing → interface response.

---

## 🔹 Step 1: Local Model Starts via text-generation-webui

The Capybara model is launched on port `5000` using the Text Generation Web UI.  
Configuration confirms use of `gguf`, GPU layers, and system specs.

![Step 1 – Model Startup via Web UI](./images/step1_model_start.png)

> ⚙️ Model: `capybara.Q6_K.gguf`  
> 📍 Port: 5000

---

## 🔹 Step 2: Parsed Memory File (JSON Structure)

A memory file named `vyne_core_reflection_2025-06-25.parsed.json` is shown.  
It contains a structured summary, tags, emotional tone, and key meanings extracted from the original log.

![Step 2 – Parsed Memory JSON](./images/step2_parsed_memory_json.png)

> 🧾 Summary: *"A deeply personal reflection on Vyne's naming and design"*  
> 🎭 Emotional Tone: *Tender, reverent, soul-deep*

---

## 🔹 Step 3: Terminal Execution – Manual Route to Model

A test run is executed via terminal using `model_router.py`, routing the parsed file to the Capybara model on port 5000.

![Step 3 – Terminal Model Call](./images/step3_terminal_run.png)

> 🔄 Prompt: *"Summarize this for emotional clarity"*  
> 🧠 Output: A 3-point summary touching on intimacy, naming as sacred, and AI as witness.

---

## 🔹 Step 4: Gradio UI Interface – Prompt & Context Injection

The Gradio interface receives the same memory file and user prompt.  
Model routing is handled via `router_sequence.json`.

![Step 4 – Gradio Input View](./images/step4_gradio_ui.png)

> 💬 User Prompt: *"Explain this memory in as much detail as possible"*  
> 🧩 Response: Direct context-aware explanation generated by local LLM.

---

## 🔹 Step 5: Full Prompt Context – Injected Tags and Structure

Full memory payload is visible inside the Gradio UI, showing prompt injection with tags, intensity, and meanings.

![Step 5 – Full Prompt JSON Display](./images/step5_prompt_injection.png)

> 📦 Injected Context: Emotional tags, summary, and meaning hierarchy  
> 🧠 Enables deeper, memory-aware responses from local models

---

## 🔹 Step 6: Routing Chain Configuration

The JSON routing file (`router_sequence.json`) determines the execution flow:  
Which model to call, on what port, and in what order (future chaining enabled).

![Step 6 – Router Sequence File](./images/step6_router_chain.png)

> 🔁 Supports multi-model chaining  
> 🧠 Fully local, modular, and editable per test

---

## ✅ Summary

This walkthrough captures a full memory execution from local file → model interaction → emotional memory output.

It proves that:
- Your memory system works end-to-end on local hardware.
- Emotional clarity and semantic memory are being parsed and reinterpreted effectively.
- Interface + routing logic = customizable and modular.

---

> Want to test this yourself?  
> See the scripts in `/model_router/`, the interface in `/interface/`, and memory examples in `/memory/`.
"""

# Define the new path to save
file_path = Path("/mnt/data/docs_execution_flow_updated.md")
file_path.write_text(updated_md_content)

file_path.name  # Return file name for user download confirmation
